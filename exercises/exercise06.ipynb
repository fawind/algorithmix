{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 (Boruvka in $O(m \\log n)$)\n",
    "\n",
    "## a) Zeige, dass while-Schleife nur höchstens $\\log n$ Iterationen hat\n",
    "\n",
    "Da in jeder Iteration der while-Schleife jeder Baum mit einem anderen Baum zusammengefügt wird, halbiert sich im schlimmsten Fall die Anzahl der Bäume in jeder Iteration. Dieser schlimmste Fall tritt ein, wenn immer genau zwei Bäume paarweise zusammengefügt werden. Da sich in diesem Fall in jeder Iteration die Anzahl der Bäume halbiert, ist dies nur maximal $\\log n$ mal möglich.\n",
    "\n",
    "## b) Durchlauf der while-Schleife in $O(m)$\n",
    "\n",
    "Zum Verwalten der Bäume wird eine Union-Find Datenstruktur verwendet (mit Pfad-Kompression und Rang). Diese wird zu Beginn des Algorithmus mit jeder Node als seperates Set initialisiert.  \n",
    "In jeder Iteration der while-Schleife initialisieren wir zudem eine Map von jedem Baum in F zu der kürzesten Kante, die aus dem Baum hinausragt. Dazu reicht es, wenn wir einmal über jede der $m$ Kanten iterieren. Dabei testen wir für jeden der beiden Endpunkte der Kante mittels Find, ob diese im selben Baum liegen. Wenn nicht, aktualisieren wir unser minimales Kantengewicht für den Baum, falls das aktuelle Kantengewicht kleiner als unser aktuell in der Map hinterlegte ist.  \n",
    "Das Aktualisieren unserer Map geht in $O(1)$. Unsere beiden Find operationen sind in amortisiert $O(\\alpha(n))$ möglich.  \n",
    "Insgesamt haben wir also für eine Iteration der while-Schleife eine Laufzeit von $O(m * \\alpha(n)) \\sim O(m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2 (Jarnik/Prim in $O(m \\log n)$)\n",
    "\n",
    "## a) Jarnik/Prim mit Priority Queue\n",
    "\n",
    "Die Priority Queue hält unsere Nodes, priorisiert nach ihrer kürzester Kante. Zu Beginn initialisieren wir die Queue mit allen Nodes, wobei ihre kürzeste Kante auf \"unendlich\" gesetzt wird. Weiterhin initialisieren einen leeren Baum $F$.  \n",
    "In jeder Iteration unserer while-Schleife können wir nun mittels `extractMin()` den Knoten $k$ mit der kürzesten Kante bekommen (initial erhalten wir unseren willkürlichen Startknoten $s$). Nun iterieren wir über alle Nachbarn $n$ von $k$ und rufen `decreaseKey(n, Kantenlänge von (k, n))` auf (Annahme: decreaseKey macht nichts, wenn der übergebene Key größer als der aktuelle ist). Anschließend wird $k$ zu unserem Baum hinzugefügt.\n",
    "\n",
    "## b) Heap insert und decreaseKey in $O(\\log n)$\n",
    "\n",
    "Gezeigt werden beide Algorithmen für einen Min-Heap. Für einen Max-Heap funktionieren diese jedoch gleich, es müssen lediglich die Vergleiche getauscht oder alle Werte invertiert werden.\n",
    "\n",
    "### I. `insert(x)` in $O(\\log n)$\n",
    "\n",
    "Zum Einfügen eines Keys in einen Binären Min-Heap hängen wir zunächst den Key an die unterste rechte freie Stelle an (bei Array implementierung: ans Ende des Arrays). Im Grunde wollen wir nun diesen Key nach oben hin \"versickern\" lassen. Dabei vertauschen wir solange den Key mit seinem Parent, bis der Key entweder die Wurzel geworden ist, oder bis der Wert des Parents kleiner-gleich dem Wert unseres Keys ist. Somit wird der Key an die Position getauscht, an der sein Parent kleiner aber seine Kinder größer sind als er. Somit gilt die Min-Heap Eigenschaft für unseren Key. Auch für die getauschten Keys kann die Heap-Eigenschaft nicht kaputt gehen, da wir den Key nur mit dem Parent tauschen, wenn der Parent kleiner als mein Key ist. Da der Parent bereits kleiner als seine Kinder waren, ist der Key nach dem Tausch auch garantiert kleiner als das andere Kind. Da der Heap eine maximale Tiefe von $\\log n$ hat, kann `bubbleUp` maximal $\\log n$ mal aufgerufen werden, falls der eingefügte Key die neue Root ist. Damit ist `insert(x)` in $O(\\log n)$.\n",
    "\n",
    "```\n",
    "insert(x):\n",
    "  1. Füge node n mit key x an das Ende des Heaps an\n",
    "  2. bubbleUp(n)\n",
    "  \n",
    "bubbleUp(n):\n",
    "  1. wenn n ist root oder parent.key <= n.key:\n",
    "  2.   return\n",
    "  3. tausche n und parent\n",
    "  4. bubbleUp(n)\n",
    "```\n",
    "\n",
    "### II. `decreaseKey(x, a)` in $O(\\log n)$\n",
    "\n",
    "Zusätzlich zum Heap speichern wir nun einen Index (z.B. Map) von den Keys im Heap mit einem Pointer zu ihrer Node (bzw. Index im Falle eines Arrays). Um den Index aktuell zu halten, aktualisieren wir diesen immer wenn wir eine neue Node einfügen, eine Node löschen oder zwei Nodes vertauschen ($O(1)$ Aufwand).\n",
    "\n",
    "Wird nun `decreaseKey(x, a)` aufgerufen, so erhalten wir in $O(1)$ eine Referenz auf die Node des Keys $x$ und können falls $a$ kleiner als der aktuelle Wert ist, diesen auf $a$ herabsetzen. Nun müssen wir noch sicherstellen, das wir die Heap-Eigenschaft wiederherstellen. Im Falle eines Min-Heaps, kann dies mit dem bereits oben definierten `bubbleUp` erreicht werden. Da wie zuvor die maximale Tiefe unseres Heaps $\\log n$ ist, kann `bubbleUp` maximal $\\log n$ mal aufgerufen werden, nämlich wenn der Key einer Leaf-Node auf einen Wert kleiner als die aktuelle Root herabgesetzt wird. Dadurch ist auch `decreaseKey(x, a)` in $O(\\log n)$.\n",
    "\n",
    "```\n",
    "decreaseKey(x, a):\n",
    "  1. node = index[x]\n",
    "  2. node.key = a if a < node.key \n",
    "  3. bubbleUp(node)\n",
    "```\n",
    "\n",
    "## c) Jarnik/Prim in $O(m \\log n)$\n",
    "\n",
    "Zu beginn fügen wir jede der $n$ Nodes in unsere Queue ein. Da alle die selbe Priorisierung haben und somit die Reihenfolge egal ist, können wir dies sogar in $O(n)$ machen.\n",
    "\n",
    "Da im schlimmsten Fall unser Spannbaum alle Kanten enthält, ergibt dies maximal $m$ Iterationen unserer while-Schleife.\n",
    "\n",
    "In der while-Schleife machen wir dann in jeder Iteration einmal `extractMin()`, was in $O(1)$ geht, sowie für jeden Nachbarn der extrahierten Node `decreaseKey()`, was in $O(\\log n)$ geht. Insgesamt lässt sich aber beobachten, dass `decreaseKey` auf jeder Kante nur maximal zweimal aufgerufen werden kann, da jede Kante nur mit zwei Knoten verbunden ist. Dadurch haben wir im Gesamtverlauf unseres Algorithmus nur $O(\\log n)$ Aufwand für `decreaseKey()` (Bzw `decreaseKey(x, a)` hat nur $O(1)$ aufwand, wenn $a$ größer-gleich dem aktuellen Wert von $x$ ist).\n",
    "Insgesamt ergibt sich somit eine Laufzeit in $O(m \\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 3 (Tiefe der Bäume bei Disjoint-Set-Forest)\n",
    "\n",
    "**Zu Zeigen:** Bäume mit $n$ Elementen in einem Disjoint-Set-Forest haben eine maximale Tiefe von $\\log n$. \n",
    "\n",
    "Beim `Union` von zwei Bäumen $A$ und $B$ können folgende Fälle auftreten:\n",
    "\n",
    "1. $Tiefe(A) > Tiefe(B)$: Da immer der kürzere Baum an den längeren Baum gehangen wird, ist der resultierende Baum $Tiefe(A)$ groß. Die lässt sich damit zeigen, dass $B$ mindestens um eins weniger tief als $A$ ist. Beim zusammenhängen mit $A$ bekommt $B$ zwar eine neue Root, dadurch wird seine Tiefe aber nur um eins erhöht, wodurch er im besten Fall genau so groß wie $A$ ist. Durch solche Operationen fügen wir also nur weitere Nodes in den Baum ein, ohne seine Tiefe zu erhöhen. Haben wir nur solche Operationen (ab $Tiefe(A)$ und $Tiefe(B)$ mindestens zwei), so können wir beliebig viele Nodes in den Baum einfügen, ohne seine Tiefe zu verändern. Dieser Fall hat also keine Auswirkungen auf unser Theorem, da wir hier immer bei $n$ Nodes eine maximale Tiefe kleiner gleich $\\log n$ erreichen können. Wichtiger ist der zweite Fall.\n",
    "\n",
    "2. $Tiefe(A) = Tiefe(B)$: Fügen wir diese beiden Bäume zusammen, so ist die neue Tiefe des Baumes um eins größer als die von $A$ und $B$. Dies liegt daran, dass einer der beiden Bäume an die Root Node des anderen gehangen wird, wodurch sich seine maximale Tiefe um eins erhöht.  \n",
    "Insgesamt lässt sich als Annahme aus Fall 1. feststellen, dass wir immer beliebig viele Nodes in unseren Baum einfügen können, ohne seine maximale Tiefe zu ändern. Um unser Theorem zu beweisen, interessieren wir uns also für den interessanten Fall, dass wir zwei Bäume mit minimaler Anzahl an Nodes haben, die beide die selbe Tiefe haben. Trivialer weise müssen beide Bäume die selbe minimale Anzahl an Nodes für die selbe Tiefe haben.  \n",
    "Fügen wir nun zwei dieser minimalen Bäume mit gleicher Tiefe zusammen, so lässt sich Folgern, dass sich die Anzahl unserer Nodes verdoppelt, während die Tiefe unseres Baumes nur um eins gewachsen ist. Verallgemeinert man dies nun für alle Bäume, so lässt sich sagen, dass wenn sich durch ein `Union` die Tiefe meines Baumes um eins erhöht, sich die Anzahl der Nodes in dem Baum um mindestens verdoppelt.  \n",
    "Aus diesem logarithmischen Verhältnis aus Tiefe und Anzahl der Nodes folgt, dass ein Baum mit $n$ Nodes eine maximale Tiefe von $\\log n$ hat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 4 (Untere Schranke für Disjoint-Set-Forest)\n",
    "\n",
    "Bei $n$ Nodes können maximal $n - 1$ Union Operationen gemacht werden. Maximal kann danach mein Baum die Größe $\\log n$ haben, wenn ich immer paarweise Union auf zwei Bäumen der selben Größe aufrufe. Da $m \\geq 2n$ bleiben mir nach diesen $n - 1$ Union Operationen noch abgerunded $\\frac{m}{2}$ meiner Anfangs $m$ Operationen übrig. Da unser Baum nun die Tiefe $\\log n$ hat, gibt es eine Find Operation, die in $\\Theta(\\log n)$ läuft (die worst-case Find Operation). Da ich noch $\\frac{m}{2}$ Operationen übrig habe, kann ich das worst-case Find $\\frac{m}{2}$ oft ausführen. Somit erhalte ich für die gesamte Operationenfolge eine untere Schranke von $\\Omega(\\frac{m}{2} \\log n) = \\Omega(m \\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 5 (Bahnproblem)\n",
    "\n",
    "Annahme: Zusatzspeicher und Initialer Aufwand nicht begrenzt.\n",
    "\n",
    "## a) `schließen`, `nächsterHalt` in $O(\\log n)$\n",
    "\n",
    "Siehe **b)**. Alternative, $Z$ nach Index sortieren und mittels Binary Search die Haltestelle zum löschen oder den Index der nächsten größeren Haltestelle suchen.\n",
    "\n",
    "## b) `schließen` in $O(1)$, `nächsterHalt` in $O(\\log n)$\n",
    "\n",
    "Zum Speichern der offenen Bahnhöfe, wird der Index jedes offenen Bahnhofes in einem balancierten Suchbaum gespeichert (z.B. Rot-Schwarz-Baum). Bei der Funktion `schließen(i)` wird nun lediglich `Z[i]` auf `0` gesetzt, was in $O(1)$ geht. Wird nun `nächsterHalt(i)` angefragt, prüfen wir zuerst in $O(1)$, ob $i$ in $Z$ bereits als gelöscht markiert ist. Ist dies nicht der Fall, so können wir einfach $i$ zurück geben, da dies ein offener Bahnhof mit index $\\geq i$ ist. Ist $i$ bereits gelöscht, müssen wir unseren Suchbaum nach dem Nachfolger von $i$ durchsuchen. Dies ist in einem balancierten Suchbaum in $O(\\log n)$ möglich. \n",
    "Bei jedem Schritt der Traversierung prüfen wir zudem, ob die aktuelle Node bereits in $Z$ als gelöscht markiert wurde. Ist dies der Fall, löschen wir die Node auch aus unserem Suchbaum, was in $O(\\log n)$ geht. Anschließend suchen wir weiter nach dem Nachfolger von $i$. Da jede Node jedoch nur maximal einmal aus dem Suchbaum gelöscht werden kann, ist die amortisierte Laufzeit für eine Suchanfrage mit eventuellem Löschen immer noch in $O(\\log n)$. Somit ist `nächsterHalt(i)` in amortisiert $O(\\log n)$ möglich.\n",
    "\n",
    "## c) `schließen` in $O(\\log n)$, `nächsterHalt` in $O(1)$\n",
    "\n",
    "Hier kann ein Shallow Tree benutzt werden, um offene Haltestellen zu verwalten. Da initial alle Haltestellen offen sind, ist jede Haltestelle ein eigener Baum in unserem Shallow Tree. Wird nun `schließen(i)` auf der Haltestelle $i$ aufgerufen, so wird mittels `find(i+1)` der Wald (Gruppe an Haltestellen) gefunden, zu der $i$ hinzugefügt wird. Auf die beiden Wälder wird dann `union` aufgerufen. Insgesamt zeigt damit der ganze Wald auf die Root Node, die die nächste offene Haltestelle repräsentiert. Wie beim Shallow Forest üblich, mache ich beim `union` zudem direkt eine Pfadkompression. Insgesamt geht das `union` beim Shallow Forest in $O(\\log n)$, wodurch auf `schließen` in $O(\\log n)$ ist. Durch die Pfadkompression bleiben meine `finds` in $O(1)$, da hier für jede Node lediglich der Parent aufgerufen werden muss um an die Root Node zu kommen. Dadurch bleibt auch `nächsterHalt(i)` in $O(1)$, da dies nur noch `find` auf der Node $i$ aufrufen muss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
